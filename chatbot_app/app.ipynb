{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR6SLc960Iq6",
        "outputId": "a76c1bfc-2eb2-4438-b436-c1b6f464f2c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "import keras\n",
        "import pickle\n",
        "import nltk\n",
        "import flask"
      ],
      "metadata": {
        "id": "J9sddP5CyslY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "xRSuupoG14Gt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIRECTORY_PATH = '/content/drive/MyDrive/chatbot/chatbot_app'"
      ],
      "metadata": {
        "id": "ScE0qD2k2CjV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = open(os.path.join(DIRECTORY_PATH, 'data.json')).read()\n",
        "intents = json.loads(data_file)"
      ],
      "metadata": {
        "id": "WnZNaPxh18-7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "\n",
        "ignore_words = ['?', '!']\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        #tokenize each word\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)\n",
        "        #add documents in the corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "# lemmaztize and lower each word and remove duplicates\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "# documents = combination between patterns and intents\n",
        "print (len(documents), \"documents\")\n",
        "# classes = intents\n",
        "print (len(classes), \"classes\", classes)\n",
        "# words = all words, vocabulary\n",
        "print (len(words), \"unique lemmatized words\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZSy9SdIDfP8",
        "outputId": "3d4fd3d2-c333-4597-9194-707ea8750eb8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47 documents\n",
            "9 classes ['adverse_drug', 'blood_pressure', 'blood_pressure_search', 'goodbye', 'greeting', 'hospital_search', 'options', 'pharmacy_search', 'thanks']\n",
            "88 unique lemmatized words [\"'s\", ',', 'a', 'adverse', 'all', 'anyone', 'are', 'awesome', 'be', 'behavior', 'blood', 'by', 'bye', 'can', 'causing', 'chatting', 'check', 'could', 'data', 'day', 'detail', 'do', 'dont', 'drug', 'entry', 'find', 'for', 'give', 'good', 'goodbye', 'have', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'history', 'hola', 'hospital', 'how', 'i', 'id', 'is', 'later', 'list', 'load', 'locate', 'log', 'looking', 'lookup', 'management', 'me', 'module', 'nearby', 'next', 'nice', 'of', 'offered', 'open', 'patient', 'pharmacy', 'pressure', 'provide', 'reaction', 'related', 'result', 'search', 'searching', 'see', 'show', 'suitable', 'support', 'task', 'thank', 'thanks', 'that', 'there', 'till', 'time', 'to', 'transfer', 'up', 'want', 'what', 'which', 'with', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(words,open(os.path.join(DIRECTORY_PATH, 'texts.pkl'),'wb'))\n",
        "pickle.dump(classes,open(os.path.join(DIRECTORY_PATH,'labels.pkl'),'wb'))"
      ],
      "metadata": {
        "id": "mE7XPyd_E8Xf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model(os.path.join(DIRECTORY_PATH, 'model.tf'))"
      ],
      "metadata": {
        "id": "lean4icZCTqn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pldrphb9CcD6",
        "outputId": "86540327-29f5-4957-ccb9-78e658516705"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Chatbot_Model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization (TextVec  (None, 50)               0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding_7 (Embedding)     (None, 50, 300)           1189800   \n",
            "                                                                 \n",
            " conv1d_14 (Conv1D)          (None, 46, 64)            96064     \n",
            "                                                                 \n",
            " max_pooling1d_7 (MaxPooling  (None, 9, 64)            0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " conv1d_15 (Conv1D)          (None, 5, 64)             20544     \n",
            "                                                                 \n",
            " global_max_pooling1d_7 (Glo  (None, 64)               0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 4)                 260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,310,828\n",
            "Trainable params: 1,310,828\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "words   = pickle.load(open(os.path.join(DIRECTORY_PATH, 'texts.pkl'), 'rb'))\n",
        "classes = pickle.load(open(os.path.join(DIRECTORY_PATH, 'labels.pkl'), 'rb'))"
      ],
      "metadata": {
        "id": "Dsb-pQ93C7Yu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('popular')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_up_sentence(sentence):\n",
        "    # tokenize the pattern - split words into array\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stem each word - create short form for word\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "\n",
        "def bow(sentence, words, show_details=True):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # bag of words - matrix of N words, vocabulary matrix\n",
        "    bag = [0]*len(words)  \n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                # assign 1 if current word is in the vocabulary position\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "    return(np.array(bag))\n",
        "\n",
        "def predict_class(sentence, model):\n",
        "    # filter out predictions below a threshold\n",
        "    #p = bow(sentence, words,show_details=False)\n",
        "    res = model.predict(np.array([sentence]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "    return return_list\n",
        "\n",
        "def getResponse(ints, intents_json):\n",
        "    tag = ints[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "    for i in list_of_intents:\n",
        "        if(i['tag'] == tag):\n",
        "            result = random.choice(i['responses'])\n",
        "            break\n",
        "    return result\n",
        "\n",
        "def chatbot_response(msg):\n",
        "    ints = predict_class(msg, model)\n",
        "    res = getResponse(ints, intents)\n",
        "    return res\n",
        "\n",
        "\n",
        "from flask import Flask, render_template, request\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.static_folder = 'static'\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/get\")\n",
        "def get_bot_response():\n",
        "    userText = request.args.get('msg')\n",
        "    return chatbot_response(userText)"
      ],
      "metadata": {
        "id": "AgaHf1l03vb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsEyskReCq3f",
        "outputId": "c1c6923d-d73d-47dd-826d-6d224b3b708a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autocorrect"
      ],
      "metadata": {
        "id": "AFMF1nVn_5FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install textdistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KmYhsSkAO-Y",
        "outputId": "f331620c-4f55-4558-e0e0-59e189e8efe4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textdistance\n",
            "  Downloading textdistance-4.5.0-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: textdistance\n",
            "Successfully installed textdistance-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install autocorrect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6LQzRkkAWiq",
        "outputId": "7981b881-c59f-4b7d-8e92-76bdb341b59f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[K     |████████████████████████████████| 622 kB 5.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622381 sha256=9f33f60e9e9b0573a4db515491191a020c22c4034ab5cbee1cacf94d088c691a\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/d4/37/8244101ad50b0f7d9bffd93ce58ed7991ee1753b290923934b\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textdistance\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "words = []\n",
        "file_name_data = ['print document', 'i printed docuemnts', 'thank you', 'hello', 'hey']\n",
        "with open(os.path.join(DIRECTORY_PATH, 'autocorrect.txt'), 'r') as f:\n",
        "    file_name_data = f.read()\n",
        "    file_name_data = file_name_data.lower()\n",
        "    words = re.findall('\\w+',file_name_data)\n",
        "# This is our vocabulary\n",
        "V = set(words)\n",
        "print(f\"The first ten words in the text are: \\n{words[0:10]}\")\n",
        "print(f\"There are {len(V)} unique words in the vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "WPSejlVJ_4ik",
        "outputId": "39a0aecb-2761-4728-8f96-d57813bc0bbc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e142c1cc7e4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfile_name_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'print document'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'i printed docuemnts'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'thank you'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hello'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hey'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIRECTORY_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'autocorrect.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mfile_name_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfile_name_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_name_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/chatbot/chatbot_app/autocorrect.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq_dict = {}  \n",
        "word_freq_dict = Counter(words)\n",
        "print(word_freq_dict.most_common()[0:10])"
      ],
      "metadata": {
        "id": "gdqEarbn_82q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = {}     \n",
        "Total = sum(word_freq_dict.values())    \n",
        "for k in word_freq_dict.keys():\n",
        "    probs[k] = word_freq_dict[k]/Total"
      ],
      "metadata": {
        "id": "qqRlLF3MAAUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_autocorrect(input_word):\n",
        "    input_word = input_word.lower()\n",
        "    if input_word in V:\n",
        "        return('Your word seems to be correct')\n",
        "    else:\n",
        "        similarities = [1-(textdistance.Jaccard(qval=2).distance(v,input_word)) for v in word_freq_dict.keys()]\n",
        "        df = pd.DataFrame.from_dict(probs, orient='index').reset_index()\n",
        "        df = df.rename(columns={'index':'Word', 0:'Prob'})\n",
        "        df['Similarity'] = similarities\n",
        "        output = df.sort_values(['Similarity', 'Prob'], ascending=False).head()\n",
        "        return(output)"
      ],
      "metadata": {
        "id": "3hnOIqZoADDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autocorrect import Speller\n",
        "\n",
        "spell = Speller(lang='en')\n",
        "\n",
        "misspelled = ['fect', 'docmnt', 'pront','cpy']\n",
        "for word in misspelled:\n",
        "    print(\"original word: \"+ word)\n",
        "    print(\"corrected word: \"+ spell(word))\n",
        "    print('')"
      ],
      "metadata": {
        "id": "W7OiybPOAI4c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}