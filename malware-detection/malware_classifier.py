import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os
import copy
import time
import datetime
import warnings

from sklearn.model_selection import train_test_split # for splitting the data into train and test samples
from sklearn import tree # for decision tree models

# for model evaluation metrics
from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix
from sklearn.metrics import average_precision_score, precision_recall_curve, average_precision_score
from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef

import plotly.express as px  # for data visualization
import plotly.graph_objects as go # for data visualization
import graphviz # for plotting decision tree graphs

from sklearn.metrics import precision_recall_curve, roc_curve, auc
from sklearn.preprocessing import label_binarize

from sklearn.model_selection import learning_curve
from sklearn.model_selection import ShuffleSplit
from sklearn.pipeline import make_pipeline

from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted
from sklearn.utils.multiclass import unique_labels
from sklearn.metrics import euclidean_distances
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn import tree
from sklearn.neighbors import KNeighborsClassifier

from xgboost.sklearn import XGBClassifier
from sklearn.neural_network import MLPClassifier

from sklearn.multiclass import OneVsRestClassifier

import xgboost as xgb

class MalwareClassifier(ClassifierMixin, BaseEstimator):
    """ An example classifier which implements a 1-NN algorithm.
    For more information regarding how to build your own classifier, read more
    in the :ref:`User Guide <user_guide>`.
    Parameters
    ----------
    demo_param : str, default='demo'
        A parameter used for demonstation of how to pass and store paramters.
    Attributes
    ----------
    X_ : ndarray, shape (n_samples, n_features)
        The input passed during :meth:`fit`.
    y_ : ndarray, shape (n_samples,)
        The labels passed during :meth:`fit`.
    classes_ : ndarray, shape (n_classes,)
        The classes seen at :meth:`fit`.
    """
    def __init__(self, outils, target_classes, class_code, models, malware_features, feature_classes, nb_classes, nb_sub_classes, demo_param='demo', random_state=23, verbose=0, epochs=100, prediction='mean'):
        self.demo_param = demo_param
        self.is_fitted_ = False
        self.random_state_ = random_state
        self.verbose_ = verbose
        self.epochs_ = epochs
        self.prediction_ = prediction

        self.outils_ = outils
        self.target_classes_ = target_classes
        self.class_code_ = class_code
        self.malware_features_ = malware_features
        self.feature_classes_ = feature_classes
        self.nb_classes_ = nb_classes
        self.nb_sub_classes_ = nb_sub_classes

        self.model_names_ = models
        self.def_models_()

        self.nb_levels_ = max(list(self.models_.keys())) + 1
        
        self.xgb_results_ = {}

    def create_model_(self, model_name, nb_classes):
        if model_name == 'svm':
            return SVC(kernel='linear', C=1, probability=True)
        elif model_name == 'knn':
            return OneVsRestClassifier(KNeighborsClassifier(n_neighbors = 5))
        elif model_name == 'xgb':
            return OneVsRestClassifier(XGBClassifier(
                learning_rate=0.1,
                n_estimators=1000,
                max_depth=5,
                min_child_weight=1,
                gamma=0,
                subsample=0.8,
                colsample_bytree=0.8,
                objective='multi:softmax',
                nthread=4,
                num_class=nb_classes,
                seed=self.random_state_
            ))
        elif model_name == 'cart':
            return OneVsRestClassifier(tree.DecisionTreeClassifier(criterion='gini', 
                splitter='best', 
                max_depth=3,
                class_weight=None,
                min_samples_leaf=1000, 
                random_state=self.random_state_
            ))
        elif model_name == 'random_forest':
            return OneVsRestClassifier(RandomForestClassifier(criterion='entropy', random_state=self.random_state_))
        elif model_name == 'mlp':
            return OneVsRestClassifier(MLPClassifier(hidden_layer_sizes=(6,5),
                    random_state=5,
                    verbose=True,
                    learning_rate_init=0.01))
        else:
            warnings.warn('[Malware Classifier] Warning: `model_name` not known. `xgb` will be used')
            return self.create_model_('xgb', nb_classes)

    def def_models_(self):
        if len(self.model_names_) == 0:
            warnings.warn('[Malware Classifier] ERROR: `models` should have at least one model')
            raise SystemExit(1)

        self.models_ = {}
        self.model_classes_ = {}
        nb_architecture_levels = max(list(self.model_names_.keys())) + 1
        for level in range(nb_architecture_levels):
            model_names = list(self.model_names_[level].values())
            if np.any(list(map(lambda x: len(x) != 0, model_names))):
                self.models_[level] = {}
                self.model_classes_[level] = {}
                for class_name, model_names in self.model_names_[level].items():
                    self.models_[level][class_name] = []
                    for i, model_name in enumerate(model_names):
                        if model_name == '':
                            warnings.warn('[Malware Classifier] Warning: no `model` was specified for `{}`. xgb was selected by default.'.format(class_name))
                            model_name = 'xgb'
                            self.model_names_[level][class_name][i] = 'xgb'
                        self.models_[level][class_name].append(self.create_model_(model_name, self.nb_sub_classes_[class_name + '_' + str(level)]))
                        #self.model_classes_[level] = self.class_code_[self.malware_features_[level]]
                        self.model_classes_[level][class_name] = self.class_code_[class_name + '_' + str(level)]
            else:
                last_level_classes = self.feature_classes_[len(self.malware_features_)-1]
                final_classes = {}
                for class_, code_ in self.feature_classes_[level-1].items():
                    if (code_ not in list(last_level_classes.values())) and (class_ not in list(last_level_classes.keys())):
                        final_classes[class_] = code_
                final_classes.update(last_level_classes)
                self.model_classes_[level-1] = final_classes#max(list(self.model_names_.keys()))]
                break
            
    def create_data_sets_(self, X, y):
        X_ = np.copy(X)
        y_ = np.copy(y)
        y_ = y_.reshape((-1,1))
        if len(self.malware_features_) > 1:
            X_ = X[:,:-(len(self.malware_features_)-1)]
            y_ = np.concatenate((X[:,-(len(self.malware_features_)-1):], y_), axis=1)
        data_sets = {}

        if self.nb_levels_ > len(self.malware_features_):
            warnings.warn('[Malware Classifier] ERROR: not enough `malware_features`')
            raise SystemExit(1)

        nb_levels = max(list(self.model_names_.keys())) + 1

        if self.nb_levels_ > nb_levels:
            warnings.warn('[Malware Classifier] ERROR: `self.nb_levels_` > `nb_levels`')
            raise SystemExit(1)
        elif (self.nb_levels_ < nb_levels) and (len(self.models_[self.nb_levels_ - 1]) > 1):
            warnings.warn('[Malware Classifier] ERROR: only one model should be defined as the last model when `self.nb_levels_` < `nb_levels`')
            raise SystemExit(1)
        
        for level in range(self.nb_levels_):
            data_sets[level] = {}

            if level == (self.nb_levels_ - 1):
                feature_level = (len(self.malware_features_) - 1)
            else:
                feature_level = level

            last_class_name = self.malware_features_[level]
            for class_name in self.models_[level].keys():
                data_sets[level][class_name] = {'X': np.copy(X_), 'y': np.copy(y_)[:, feature_level]}
                #data_sets[level][class_name]['y'] = pd.Series(data_sets[level][class_name]['y']).map(self.class_code_[class_name + '_' + str(level)]).to_numpy()
                
                if level > 0:
                    #class_code = self.class_code_[self.malware_features_[level - 1]][class_name]
                    class_code = self.model_classes_[level - 1][list(self.models_[level-1].keys())[0]][class_name]
                    data_sets[level][class_name]['X'] = data_sets[level][class_name]['X'][y_[:,level - 1] == class_code,:]
                    data_sets[level][class_name]['y'] = data_sets[level][class_name]['y'][y_[:,level - 1] == class_code]

                last_class_name = class_name

        return data_sets

    def get_xgb_results_(self, X, y, classification_type):
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, stratify=y, random_state=self.random_state_, test_size=0.2
        )

        feature_names = None
        if isinstance(X, pd.DataFrame):
            feature_names = X.columns.values
        
        train = xgb.DMatrix(X_train, label=y_train, feature_names=feature_names)
        val = xgb.DMatrix(X_val, label=y_val, feature_names=feature_names)

        params = self.models_[classification_type].get_xgb_params()
        metrics = ['mlogloss', 'merror']
        params['eval_metric'] = metrics

        store = {}
        evallist = [(val, 'val'), (train,'train')]
        
        xgb.train(params, train, self.epochs, evallist, evals_result=store, verbose_eval=100)

        self.xgb_results_[classification_type] = {'metrics': metrics, 'store': store, 'epochs': self.epochs}

    def fit(self, X, y):
        """A reference implementation of a fitting function for a classifier.
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The training input samples.
        y : array-like, shape (n_samples,)
            The target values. An array of int.
        Returns
        -------
        self : object
            Returns self.
        """
        # Check that X and y have correct shape
        X, y = check_X_y(X, y)
        # Store the classes seen during fit
        self.classes_ = unique_labels(y)

        start_fitting = time.time()

        data_sets = self.create_data_sets_(X, y)
        
        for level in range(self.nb_levels_):
            for class_name, dataset in data_sets[level].items():
                s = '' if (len(self.model_names_[level][class_name]) == 1) else 's'
                print('{}: {} model{}'.format(class_name, len(self.model_names_[level][class_name]), s))
                for i in range(len(self.model_names_[level][class_name])):
                    print('\t{} fitting ...'.format(self.model_names_[level][class_name][i]))
                    self.models_[level][class_name][i].fit(dataset['X'], dataset['y'])

                    if (self.verbose_ == 2) and (self.model_names_[level][class_name][i] == 'xgb'):
                        self.get_xgb_results_(X, y, class_name)
                    
                print('done\n')

        end_fitting = time.time()
        print('elapsed time (fitting model): ', str(datetime.timedelta(seconds=(end_fitting - start_fitting))))

        self.is_fitted_ = True

        # Return the classifier
        return self

    def predict_target(self, X, class_name, level=0):
        predicted_class_codes = np.array([-1] * len(X))
        predicted_classes = np.array([''] * len(X))

        if (level == (self.nb_levels_ - 1)) and (self.nb_levels_ < len(self.malware_features_)):
            class_codes = self.model_classes_[level]
        else:
            class_codes = self.model_classes_[level][class_name]

        if len(self.models_[level][class_name]) > 1:
            

            if self.prediction_ == 'max':
                max_proba = np.zeros(len(X))
                preds = np.zeros(len(X))
                for i in range(len(self.models_[level][class_name])):
                    model_pred_proba = self.models_[level][class_name][i].predict_proba(X)
                    model_proba = np.amax(model_pred_proba, axis=1)
                    model_arg_proba = np.argmax(model_pred_proba, axis=1)
                    indexes = np.where(model_proba > max_proba)
                    max_proba[indexes] = model_proba[indexes]
                    preds[indexes] = np.array(list(class_codes.values()))[model_arg_proba[indexes]]
                
                pred_classes = preds

            elif self.prediction == 'mean':
                pred_proba = np.zeros((len(X), len(class_codes)))
                for i in range(len(self.models_[level][class_name])):
                    pred_proba = np.add(pred_proba, self.models_[level][class_name][i].predict_proba(X))
                pred_classes = np.array(list(class_codes.values()))[np.argmax(pred_proba, axis=1)]
        else:
            pred_classes = self.models_[level][class_name][0].predict(X)

        if isinstance(pred_classes, list):
            pred_classes = np.array(pred_classes)

        for target_class, class_code in class_codes.items():
            # if the predicted classes aren't the final classes
            # and if the predicted class needs to be further classified
            if (level < (self.nb_levels_ - 1)) and (target_class in list(self.models_[level+1].keys())):
                next_X = np.copy(X[pred_classes == class_code])
                if len(next_X) > 0:
                    next_class_codes, next_classes = self.predict_target(next_X, target_class, level=level+1)
                    predicted_class_codes[pred_classes == class_code] = next_class_codes
                    predicted_classes[pred_classes == class_code] = next_classes
            else:
                predicted_class_codes[pred_classes == class_code] = class_code
                predicted_classes[pred_classes == class_code] = target_class
          
        return predicted_class_codes, predicted_classes

    def predict(self, X):
        """ A reference implementation of a prediction for a classifier.
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            The input samples.
        Returns
        -------
        y : ndarray, shape (n_samples,)
            The label for each sample is the label of the closest sample
            seen during fit.
        """
        # Check is fit had been called
        check_is_fitted(self, 'is_fitted_')

        # Input validation
        X = check_array(X)

        X_ = np.copy(X)
        if len(self.malware_features_) > 1:
            X_ = X[:,:-(len(self.malware_features_)-1)]

        predicted_class_codes, predicted_classes = self.predict_target(X_, class_name=self.malware_features_[0])

        return predicted_class_codes